{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Run Llama 2 with Python\n",
        "\n",
        "This Jupyter Notebook is meant to be used in Google Colab, but can be easily run locally.  \n",
        "The aim of this notebook is to be an easy and direct way to access and start using llama 2 with python.\n",
        "\n",
        "Based on this post from SWHarden.com:\n",
        "https://swharden.com/blog/2023-07-29-ai-chat-locally-with-python/"
      ],
      "metadata": {
        "id": "_FsbMTB-4adf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8HGJ9hBcJ9We",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16a5f352-5f51-4aab-df90-5d5e5e0fd186"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python==0.1.78\n",
            "  Downloading llama_cpp_python-0.1.78.tar.gz (1.7 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.7 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.7 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.1.78) (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.1.78) (1.23.5)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python==0.1.78)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.1.78-cp310-cp310-linux_x86_64.whl size=296587 sha256=6e6d010828b71b03b29ac90ab58b8ea2b09d478a19be149a31fa95ef5cc1e4d3\n",
            "  Stored in directory: /root/.cache/pip/wheels/61/f9/20/9ca660a9d3f2a47e44217059409478865948b5c8a1cba70030\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.1.78\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-cpp-python==0.1.78"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If something goes wrong here or later, please check the most up-to-date version of llama-cpp-python at the following link:\n",
        "\n",
        "https://pypi.org/project/llama-cpp-python/"
      ],
      "metadata": {
        "id": "rDIzXW87Utxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "from IPython.display import display, HTML\n",
        "import json\n",
        "import time\n",
        "import pathlib"
      ],
      "metadata": {
        "id": "OB4xF6kR0ukS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wget"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s42STdD001z2",
        "outputId": "d5c941a0-000a-404d-fcdd-c2ba68683e29"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=b14ca093b9facff43df509c939003c8c0ed8c171941f1614a66d38be4437f64c\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we bring in model Q8_0, there are other possibilities, maybe newer models with better performance."
      ],
      "metadata": {
        "id": "0ytNxux05v_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wget\n",
        "\n",
        "url = 'https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/resolve/main/llama-2-7b-chat.ggmlv3.q8_0.bin'\n",
        "local_path = '/content/llama-2-7b-chat.ggmlv3.q8_0.bin'\n",
        "wget.download(url, local_path)\n",
        "\n",
        "MODEL_Q8_0 = Llama(\n",
        "    model_path=\"/content/llama-2-7b-chat.ggmlv3.q8_0.bin\",\n",
        "    n_ctx=2048)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gm-NmfqL02lv",
        "outputId": "a6e8e4d4-0653-42c5-f868-664ae573241a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def query(model, question):\n",
        "    model_name = pathlib.Path(model.model_path).name\n",
        "    time_start = time.time()\n",
        "    prompt = f\"Q: {question} A:\"\n",
        "    output = model(prompt=prompt, max_tokens=0) # if max tokens is zero, depends on n_ctx\n",
        "    response = output[\"choices\"][0][\"text\"]\n",
        "    time_elapsed = time.time() - time_start\n",
        "    display(HTML(f'<code>{model_name} response time: {time_elapsed:.02f} sec</code>'))\n",
        "    display(HTML(f'<strong>Question:</strong> {question}'))\n",
        "    display(HTML(f'<strong>Answer:</strong> {response}'))\n",
        "    print(json.dumps(output, indent=2))"
      ],
      "metadata": {
        "id": "uHaX5YRi1BAI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this last cell we ask the prompt with a string between quotation marks, if it's too long it's possible to surpass the token limitation, we'll be informed with an error instead of the result.  \n",
        "To solve this just reduce the prompt text."
      ],
      "metadata": {
        "id": "DlqOueQa7PMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query(MODEL_Q8_0, \"Tell me a story using 100 words\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "OBBzI_b91RF0",
        "outputId": "8cd93568-46c5-4642-ca5d-57f8d47bb610"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<code>llama-2-7b-chat.ggmlv3.q8_0.bin response time: 114.75 sec</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<strong>Question:</strong> Tell me a story using 100 words"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<strong>Answer:</strong>  Sure! Here is a story in exactly 100 words:\n",
              "\n",
              "Once upon a time, there was a little girl named Lily. She lived in a tiny house with her mother and father. One day, Lily found a hidden garden behind their house filled with the most beautiful flowers she had ever seen. She picked one and gave it to her mother, who smiled and said, \"Thank you, dear.\" From that day on, Lily tended the garden every day, and it became her happy place."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"id\": \"cmpl-77f2c6a5-1fd9-4a62-a37c-fa16657704ba\",\n",
            "  \"object\": \"text_completion\",\n",
            "  \"created\": 1694873617,\n",
            "  \"model\": \"/content/llama-2-7b-chat.ggmlv3.q8_0.bin\",\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"text\": \" Sure! Here is a story in exactly 100 words:\\n\\nOnce upon a time, there was a little girl named Lily. She lived in a tiny house with her mother and father. One day, Lily found a hidden garden behind their house filled with the most beautiful flowers she had ever seen. She picked one and gave it to her mother, who smiled and said, \\\"Thank you, dear.\\\" From that day on, Lily tended the garden every day, and it became her happy place.\",\n",
            "      \"index\": 0,\n",
            "      \"logprobs\": null,\n",
            "      \"finish_reason\": \"stop\"\n",
            "    }\n",
            "  ],\n",
            "  \"usage\": {\n",
            "    \"prompt_tokens\": 15,\n",
            "    \"completion_tokens\": 107,\n",
            "    \"total_tokens\": 122\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    }
  ]
}